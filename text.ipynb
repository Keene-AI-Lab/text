{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opensmile gdown transformers soundfile shap --quiet"
      ],
      "metadata": {
        "id": "eI40uUurY3lu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "889010f0-2254-46b9-f5a9-936a73030834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/996.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.0/996.0 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/150.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.4/138.4 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "import os\n",
        "import tempfile\n",
        "import concurrent.futures\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio, display\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import shap\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "fwv3AsVj_TnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accessing Dataset"
      ],
      "metadata": {
        "id": "nTrmLrrDYzF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install dataset using gdown\n",
        "!gdown 1p4ZQOwbHkD2RAvq2K5ekY5gcMi24XLnS\n",
        "\n",
        "tarball_filename = \"ADReSSo21-diagnosis-train.tar\"\n",
        "\n",
        "# extract contents from tarball\n",
        "with tarfile.open(tarball_filename, 'r:*') as tar:\n",
        "    tar.extractall(path=\"./\")\n",
        "\n",
        "# remove tarball after extraction\n",
        "os.remove(tarball_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pve1AehzY_WU",
        "outputId": "0e0fd198-779c-4bb3-bdc3-fbdc4c82fb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1p4ZQOwbHkD2RAvq2K5ekY5gcMi24XLnS\n",
            "From (redirected): https://drive.google.com/uc?id=1p4ZQOwbHkD2RAvq2K5ekY5gcMi24XLnS&confirm=t&uuid=b3ddf379-a455-4aa3-a747-bf6402e534b9\n",
            "To: /content/ADReSSo21-diagnosis-train.tar\n",
            "100% 1.75G/1.75G [00:18<00:00, 92.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_paths(dataset_root: str = \"ADReSSo21/diagnosis/train\"):\n",
        "    \"\"\"\n",
        "    Return a dictionary containing paths for the audio and segmentation directories.\n",
        "    \"\"\"\n",
        "    dataset_dir = Path(dataset_root)\n",
        "    return {\n",
        "        \"audio\": dataset_dir / \"audio\",\n",
        "        \"audio_ad\": dataset_dir / \"audio\" / \"ad\",\n",
        "        \"audio_cn\": dataset_dir / \"audio\" / \"cn\",\n",
        "        \"segmentation\": dataset_dir / \"segmentation\",\n",
        "        \"segmentation_ad\": dataset_dir / \"segmentation\" / \"ad\",\n",
        "        \"segmentation_cn\": dataset_dir / \"segmentation\" / \"cn\",\n",
        "    }\n",
        "\n",
        "def load_audio_file(file_path: Path):\n",
        "    \"\"\"Load the audio waveform and its sampling rate using soundfile.\"\"\"\n",
        "    return sf.read(file_path)\n",
        "\n",
        "def load_segmentation(seg_file: Path):\n",
        "    \"\"\"Load the segmentation CSV into a Pandas DataFrame.\"\"\"\n",
        "    return pd.read_csv(seg_file)\n",
        "\n",
        "def extract_patient_segments(audio: np.ndarray, sr: int, seg_df: pd.DataFrame, speaker: str = \"PAR\"):\n",
        "    \"\"\"\n",
        "    Extract patient segments from the audio as specified by the segmentation CSV.\n",
        "\n",
        "    Returns:\n",
        "      - patient_mask: Array with patient segments (NaN elsewhere).\n",
        "      - concatenated: All patient segments concatenated into a single array.\n",
        "      - segments: List of (begin_sample, end_sample) tuples.\n",
        "    \"\"\"\n",
        "    patient_df = seg_df[seg_df[\"speaker\"] == speaker]\n",
        "    patient_mask = np.full_like(audio, np.nan)\n",
        "    segments_list = []\n",
        "    segments = []\n",
        "    for _, row in patient_df.iterrows():\n",
        "        begin_sample = int(float(row[\"begin\"]) * sr / 1000)\n",
        "        end_sample = int(float(row[\"end\"]) * sr / 1000)\n",
        "        patient_mask[begin_sample:end_sample] = audio[begin_sample:end_sample]\n",
        "        segments_list.append(audio[begin_sample:end_sample])\n",
        "        segments.append((begin_sample, end_sample))\n",
        "    concatenated = np.concatenate(segments_list) if segments_list else np.array([])\n",
        "    return patient_mask, concatenated, segments\n",
        "\n",
        "def process_audio(audio_file: Path, seg_file: Path, plot: bool = False):\n",
        "    \"\"\"\n",
        "    Load an audio file and its corresponding segmentation CSV,\n",
        "    extract the patient-only segments, and optionally plot the waveform.\n",
        "\n",
        "    Returns:\n",
        "      audio, sr, patient_mask, concatenated (patient-only audio), segments.\n",
        "    \"\"\"\n",
        "    audio, sr = load_audio_file(audio_file)\n",
        "    seg_df = load_segmentation(seg_file)\n",
        "    patient_mask, concatenated, segments = extract_patient_segments(audio, sr, seg_df)\n",
        "    if plot:\n",
        "        time_axis = np.linspace(0, len(audio) / sr, num=len(audio))\n",
        "        plt.figure(figsize=(14, 4))\n",
        "        plt.plot(time_axis, audio, label=\"Original\")\n",
        "        plt.plot(time_axis, patient_mask, label=\"Patient-Only\")\n",
        "        plt.xlabel(\"Time (s)\")\n",
        "        plt.ylabel(\"Amplitude\")\n",
        "        plt.title(\"Patient Speech Isolation\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    return audio, sr, patient_mask, concatenated, segments\n",
        "\n",
        "def init_transcriber(model_name: str = \"openai/whisper-large\", device: int = 0):\n",
        "    \"\"\"\n",
        "    Initialize the Hugging Face ASR pipeline using GPU.\n",
        "\n",
        "    Set device=0 to use your GPU.\n",
        "    \"\"\"\n",
        "    return pipeline(\"automatic-speech-recognition\", model=model_name, device=device)\n",
        "\n",
        "def transcribe_audio_file(file_path: str, transcriber) -> str:\n",
        "    \"\"\"\n",
        "    Transcribe an audio file given its file path using the provided transcriber.\n",
        "\n",
        "    Returns the transcription text.\n",
        "    \"\"\"\n",
        "    result = transcriber(file_path, return_timestamps=True)\n",
        "    return result.get(\"text\", \"\")\n",
        "\n",
        "def create_transcription_df(transcription_records: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a Pandas DataFrame from a list of transcription records.\n",
        "    \"\"\"\n",
        "    return pd.DataFrame(transcription_records)\n",
        "\n",
        "# -------------------------------------------------------------------------------\n",
        "# Main Function: Get Transcripts for a Specific Group (Process All Files)\n",
        "# -------------------------------------------------------------------------------\n",
        "def get_transcripts_for_group(group: str):\n",
        "    \"\"\"\n",
        "    Process all audio files from a specified group (\"ad\" or \"cn\") on GPU,\n",
        "    print the word count for each transcript, and save the sorted transcripts\n",
        "    to a CSV file.\n",
        "\n",
        "    The CSV file is named differently based on the group.\n",
        "    \"\"\"\n",
        "    paths = get_dataset_paths()\n",
        "    device = 0  # Use GPU (device index 0)\n",
        "    model_name = \"openai/whisper-large\"\n",
        "\n",
        "    if group.lower() == \"ad\":\n",
        "        audio_files = sorted(list(paths[\"audio_ad\"].glob(\"*.wav\")), key=lambda f: f.name)\n",
        "        seg_dir = paths[\"segmentation_ad\"]\n",
        "        csv_filename = \"sorted_patient_transcriptions_ad.csv\"\n",
        "    elif group.lower() == \"cn\":\n",
        "        audio_files = sorted(list(paths[\"audio_cn\"].glob(\"*.wav\")), key=lambda f: f.name)\n",
        "        seg_dir = paths[\"segmentation_cn\"]\n",
        "        csv_filename = \"sorted_patient_transcriptions_cn.csv\"\n",
        "    else:\n",
        "        raise ValueError(\"Unknown group. Please specify 'ad' or 'cn'.\")\n",
        "\n",
        "    print(f\"Processing {len(audio_files)} audio files from the '{group.upper()}' group on GPU.\")\n",
        "    transcripts = []\n",
        "    transcriber = init_transcriber(model_name=model_name, device=device)  # Initialize once for efficiency\n",
        "\n",
        "    for audio_file in audio_files:\n",
        "        seg_file = seg_dir / f\"{audio_file.stem}.csv\"\n",
        "\n",
        "        # Process audio to extract the patient-only audio\n",
        "        audio, sr, patient_mask, concatenated, segments = process_audio(audio_file, seg_file, plot=False)\n",
        "        if concatenated.size == 0:\n",
        "            transcript = \"No patient speech segments found.\"\n",
        "        else:\n",
        "            # Write the concatenated patient-only audio to a temporary WAV file\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n",
        "                temp_filename = tmp_file.name\n",
        "            sf.write(temp_filename, concatenated, sr)\n",
        "            transcript = transcribe_audio_file(temp_filename, transcriber)\n",
        "            os.remove(temp_filename)\n",
        "\n",
        "        transcripts.append({\"file_name\": audio_file.name, \"transcription\": transcript})\n",
        "        word_count = len(transcript.split())\n",
        "        print(f\"File '{audio_file.name}': {word_count} words in transcript.\")\n",
        "\n",
        "    # Sort the transcripts by file name and save to CSV\n",
        "    transcripts = sorted(transcripts, key=lambda x: x[\"file_name\"])\n",
        "    df = create_transcription_df(transcripts)\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"All sorted transcriptions for the '{group.upper()}' group saved to {csv_filename}\")\n"
      ],
      "metadata": {
        "id": "ZHFQbN73BUaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transcription\n"
      ],
      "metadata": {
        "id": "3RKesCnlfOB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process all audio files from the AD group and save to CSV\n",
        "get_transcripts_for_group(\"ad\")\n",
        "# Process all audio files from the CN group and save to CSV\n",
        "get_transcripts_for_group(\"cn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpAM6X0FfRgy",
        "outputId": "745566db-45f0-4531-a78a-375ab47bc681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 87 audio files from the 'AD' group on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso024.wav': 182 words in transcript.\n",
            "File 'adrso025.wav': 195 words in transcript.\n",
            "File 'adrso027.wav': 83 words in transcript.\n",
            "File 'adrso028.wav': 47 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso031.wav': 90 words in transcript.\n",
            "File 'adrso032.wav': 184 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso033.wav': 72 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso035.wav': 84 words in transcript.\n",
            "File 'adrso036.wav': 40 words in transcript.\n",
            "File 'adrso039.wav': 28 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso043.wav': 250 words in transcript.\n",
            "File 'adrso045.wav': 291 words in transcript.\n",
            "File 'adrso046.wav': 108 words in transcript.\n",
            "File 'adrso047.wav': 94 words in transcript.\n",
            "File 'adrso049.wav': 151 words in transcript.\n",
            "File 'adrso053.wav': 201 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso054.wav': 93 words in transcript.\n",
            "File 'adrso055.wav': 121 words in transcript.\n",
            "File 'adrso056.wav': 289 words in transcript.\n",
            "File 'adrso059.wav': 63 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso060.wav': 46 words in transcript.\n",
            "File 'adrso063.wav': 27 words in transcript.\n",
            "File 'adrso068.wav': 62 words in transcript.\n",
            "File 'adrso070.wav': 262 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso071.wav': 87 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso072.wav': 91 words in transcript.\n",
            "File 'adrso074.wav': 103 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso075.wav': 60 words in transcript.\n",
            "File 'adrso077.wav': 63 words in transcript.\n",
            "File 'adrso078.wav': 87 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso089.wav': 111 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso090.wav': 44 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso092.wav': 75 words in transcript.\n",
            "File 'adrso093.wav': 146 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso098.wav': 111 words in transcript.\n",
            "File 'adrso106.wav': 121 words in transcript.\n",
            "File 'adrso109.wav': 106 words in transcript.\n",
            "File 'adrso110.wav': 101 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso112.wav': 76 words in transcript.\n",
            "File 'adrso116.wav': 52 words in transcript.\n",
            "File 'adrso122.wav': 30 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso123.wav': 49 words in transcript.\n",
            "File 'adrso125.wav': 86 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso126.wav': 64 words in transcript.\n",
            "File 'adrso128.wav': 25 words in transcript.\n",
            "File 'adrso130.wav': 33 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso134.wav': 31 words in transcript.\n",
            "File 'adrso138.wav': 78 words in transcript.\n",
            "File 'adrso141.wav': 49 words in transcript.\n",
            "File 'adrso142.wav': 50 words in transcript.\n",
            "File 'adrso144.wav': 78 words in transcript.\n",
            "File 'adrso187.wav': 109 words in transcript.\n",
            "File 'adrso188.wav': 52 words in transcript.\n",
            "File 'adrso189.wav': 98 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso190.wav': 194 words in transcript.\n",
            "File 'adrso192.wav': 84 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso197.wav': 91 words in transcript.\n",
            "File 'adrso198.wav': 41 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso200.wav': 170 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso202.wav': 107 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso205.wav': 105 words in transcript.\n",
            "File 'adrso206.wav': 95 words in transcript.\n",
            "File 'adrso209.wav': 71 words in transcript.\n",
            "File 'adrso211.wav': 147 words in transcript.\n",
            "File 'adrso212.wav': 100 words in transcript.\n",
            "File 'adrso215.wav': 53 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso216.wav': 80 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso218.wav': 49 words in transcript.\n",
            "File 'adrso220.wav': 90 words in transcript.\n",
            "File 'adrso222.wav': 103 words in transcript.\n",
            "File 'adrso223.wav': 72 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso224.wav': 132 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso228.wav': 79 words in transcript.\n",
            "File 'adrso229.wav': 140 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso232.wav': 98 words in transcript.\n",
            "File 'adrso233.wav': 133 words in transcript.\n",
            "File 'adrso234.wav': 100 words in transcript.\n",
            "File 'adrso236.wav': 39 words in transcript.\n",
            "File 'adrso237.wav': 72 words in transcript.\n",
            "File 'adrso244.wav': 85 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso245.wav': 56 words in transcript.\n",
            "File 'adrso246.wav': 67 words in transcript.\n",
            "File 'adrso247.wav': 94 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso248.wav': 72 words in transcript.\n",
            "File 'adrso249.wav': 57 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso250.wav': 148 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso253.wav': 99 words in transcript.\n",
            "All sorted transcriptions for the 'AD' group saved to sorted_patient_transcriptions_ad.csv\n",
            "Processing 79 audio files from the 'CN' group on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso002.wav': 171 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso003.wav': 82 words in transcript.\n",
            "File 'adrso005.wav': 141 words in transcript.\n",
            "File 'adrso007.wav': 63 words in transcript.\n",
            "File 'adrso008.wav': 137 words in transcript.\n",
            "File 'adrso010.wav': 41 words in transcript.\n",
            "File 'adrso012.wav': 137 words in transcript.\n",
            "File 'adrso014.wav': 138 words in transcript.\n",
            "File 'adrso015.wav': 99 words in transcript.\n",
            "File 'adrso016.wav': 79 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso017.wav': 117 words in transcript.\n",
            "File 'adrso018.wav': 5 words in transcript.\n",
            "File 'adrso019.wav': 5 words in transcript.\n",
            "File 'adrso021.wav': 5 words in transcript.\n",
            "File 'adrso022.wav': 5 words in transcript.\n",
            "File 'adrso023.wav': 5 words in transcript.\n",
            "File 'adrso148.wav': 240 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso151.wav': 49 words in transcript.\n",
            "File 'adrso152.wav': 130 words in transcript.\n",
            "File 'adrso153.wav': 110 words in transcript.\n",
            "File 'adrso154.wav': 60 words in transcript.\n",
            "File 'adrso156.wav': 95 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso157.wav': 128 words in transcript.\n",
            "File 'adrso158.wav': 100 words in transcript.\n",
            "File 'adrso159.wav': 57 words in transcript.\n",
            "File 'adrso160.wav': 45 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso161.wav': 74 words in transcript.\n",
            "File 'adrso162.wav': 135 words in transcript.\n",
            "File 'adrso164.wav': 173 words in transcript.\n",
            "File 'adrso165.wav': 82 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso167.wav': 151 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso168.wav': 129 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso169.wav': 63 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso170.wav': 87 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso172.wav': 133 words in transcript.\n",
            "File 'adrso173.wav': 104 words in transcript.\n",
            "File 'adrso177.wav': 108 words in transcript.\n",
            "File 'adrso178.wav': 64 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso180.wav': 102 words in transcript.\n",
            "File 'adrso182.wav': 90 words in transcript.\n",
            "File 'adrso183.wav': 192 words in transcript.\n",
            "File 'adrso186.wav': 79 words in transcript.\n",
            "File 'adrso257.wav': 92 words in transcript.\n",
            "File 'adrso259.wav': 183 words in transcript.\n",
            "File 'adrso260.wav': 84 words in transcript.\n",
            "File 'adrso261.wav': 114 words in transcript.\n",
            "File 'adrso262.wav': 67 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso263.wav': 98 words in transcript.\n",
            "File 'adrso264.wav': 89 words in transcript.\n",
            "File 'adrso265.wav': 237 words in transcript.\n",
            "File 'adrso266.wav': 72 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso267.wav': 117 words in transcript.\n",
            "File 'adrso268.wav': 156 words in transcript.\n",
            "File 'adrso270.wav': 109 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso273.wav': 101 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso274.wav': 286 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso276.wav': 493 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso277.wav': 140 words in transcript.\n",
            "File 'adrso278.wav': 44 words in transcript.\n",
            "File 'adrso280.wav': 229 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso281.wav': 123 words in transcript.\n",
            "File 'adrso283.wav': 137 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso285.wav': 32 words in transcript.\n",
            "File 'adrso286.wav': 103 words in transcript.\n",
            "File 'adrso289.wav': 74 words in transcript.\n",
            "File 'adrso291.wav': 186 words in transcript.\n",
            "File 'adrso292.wav': 59 words in transcript.\n",
            "File 'adrso296.wav': 139 words in transcript.\n",
            "File 'adrso298.wav': 92 words in transcript.\n",
            "File 'adrso299.wav': 107 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso300.wav': 54 words in transcript.\n",
            "File 'adrso302.wav': 236 words in transcript.\n",
            "File 'adrso307.wav': 170 words in transcript.\n",
            "File 'adrso308.wav': 152 words in transcript.\n",
            "File 'adrso309.wav': 104 words in transcript.\n",
            "File 'adrso310.wav': 81 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso312.wav': 112 words in transcript.\n",
            "File 'adrso315.wav': 101 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso316.wav': 88 words in transcript.\n",
            "All sorted transcriptions for the 'CN' group saved to sorted_patient_transcriptions_cn.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction"
      ],
      "metadata": {
        "id": "Pdkz9KCdSvIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# disable HF-Hub and Transformers progress bars\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "from transformers import logging\n",
        "logging.disable_progress_bar()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "\n",
        "# ─── Load transcripts & labels ──────────────────────────────────────────────\n",
        "df_ad = pd.read_csv(\"sorted_patient_transcriptions_ad.csv\")\n",
        "df_cn = pd.read_csv(\"sorted_patient_transcriptions_cn.csv\")\n",
        "\n",
        "# assume df_ad/df_cn each have columns: \"file_name\" (e.g. \"adrso024.wav\"), \"transcription\"\n",
        "texts = df_ad[\"transcription\"].tolist() + df_cn[\"transcription\"].tolist()\n",
        "y     = np.array([1] * len(df_ad) + [0] * len(df_cn))\n",
        "\n",
        "# ─── Embed with DistilBERT ─────────────────────────────────────────────────\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model     = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").eval()\n",
        "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "features = []\n",
        "with torch.no_grad():\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=True\n",
        "        ).to(device)\n",
        "        emb = (\n",
        "            model(**inputs)\n",
        "            .last_hidden_state   # (1, seq_len, hidden_dim)\n",
        "            .squeeze(0)          # (seq_len, hidden_dim)\n",
        "            .mean(dim=0)         # (hidden_dim,)\n",
        "        )\n",
        "        features.append(emb.cpu().numpy())\n",
        "\n",
        "X = np.vstack(features)"
      ],
      "metadata": {
        "id": "GuOLkLfO8cxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_cols = [f\"f{i}\" for i in range(X.shape[1])]\n",
        "df_feat   = pd.DataFrame(X, columns=feat_cols)\n",
        "\n",
        "# strip \".wav\" so file_id matches audio CSVs\n",
        "def strip_ext(fn):\n",
        "    return os.path.splitext(fn)[0]\n",
        "\n",
        "file_ids = (\n",
        "    df_ad[\"file_name\"].map(strip_ext).tolist()\n",
        "  + df_cn[\"file_name\"].map(strip_ext).tolist()\n",
        ")\n",
        "\n",
        "df_feat[\"file_id\"] = file_ids\n",
        "df_feat[\"label\"]   = y\n",
        "\n",
        "# reorder columns: file_id, label, then features\n",
        "df_feat = df_feat[[\"file_id\", \"label\"] + feat_cols]\n",
        "\n",
        "df_feat.to_csv(\"text_feat.csv\", index=False)\n",
        "\n",
        "print(\"saved text_feat.csv (clean file_id)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y8TZqdfs-tk",
        "outputId": "604cab75-0fdd-4d1e-8d2f-98a8afa56aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved text_feats.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "aMLUMPUkOcPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        ")\n",
        "from tabulate import tabulate\n",
        "\n",
        "def train_rf(df, feat_cols, split_col=\"split\",\n",
        "             train_value=\"train\", test_value=\"test\",\n",
        "             n_estimators=100, random_state=42, n_jobs=-1):\n",
        "    X_train = df[df[split_col]==train_value][feat_cols].astype(np.float32).values\n",
        "    y_train = df[df[split_col]==train_value][\"label\"].values\n",
        "    X_test  = df[df[split_col]==test_value][feat_cols].astype(np.float32).values\n",
        "    y_test  = df[df[split_col]==test_value][\"label\"].values\n",
        "\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        random_state=random_state,\n",
        "        n_jobs=n_jobs\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "def train_xgb(df, feat_cols, split_col=\"split\",\n",
        "              train_value=\"train\", test_value=\"test\",\n",
        "              n_estimators=100, max_depth=3, learning_rate=0.1,\n",
        "              random_state=42, n_jobs=-1, eval_metric=\"logloss\"):\n",
        "    X_train = df[df[split_col]==train_value][feat_cols].astype(np.float32).values\n",
        "    y_train = df[df[split_col]==train_value][\"label\"].values\n",
        "\n",
        "    clf = XGBClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        learning_rate=learning_rate,\n",
        "        random_state=random_state,\n",
        "        n_jobs=n_jobs,\n",
        "        eval_metric=eval_metric,\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "def train_mlp(df, feat_cols, split_col=\"split\",\n",
        "              train_value=\"train\", test_value=\"test\",\n",
        "              hidden_layer_sizes=(100,), activation=\"relu\",\n",
        "              solver=\"adam\", max_iter=200, random_state=42):\n",
        "    X_train = df[df[split_col]==train_value][feat_cols].astype(np.float32).values\n",
        "    y_train = df[df[split_col]==train_value][\"label\"].values\n",
        "\n",
        "    clf = MLPClassifier(\n",
        "        hidden_layer_sizes=hidden_layer_sizes,\n",
        "        activation=activation,\n",
        "        solver=solver,\n",
        "        max_iter=max_iter,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "def format_results(results):\n",
        "    headers = [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"]\n",
        "    rows = [\n",
        "        [\n",
        "            r[\"model\"],\n",
        "            f\"{r['accuracy']:.3f}\",\n",
        "            f\"{r['precision']:.3f}\",\n",
        "            f\"{r['recall']:.3f}\",\n",
        "            f\"{r['f1']:.3f}\"\n",
        "        ]\n",
        "        for r in results\n",
        "    ]\n",
        "    print(tabulate(rows, headers, tablefmt=\"pretty\"))\n",
        "\n",
        "def train_fn(df,\n",
        "             feat_cols,\n",
        "             split_col=\"split\",\n",
        "             train_value=\"train\",\n",
        "             test_value=\"test\",\n",
        "             models=None,\n",
        "             rf_params=None,\n",
        "             xgb_params=None,\n",
        "             mlp_params=None):\n",
        "    if models is None:\n",
        "        models = [\"rf\", \"xgb\", \"mlp\"]\n",
        "    rf_params  = rf_params or {}\n",
        "    xgb_params = xgb_params or {}\n",
        "    mlp_params = mlp_params or {}\n",
        "\n",
        "    test_df = df[df[split_col] == test_value]\n",
        "    X_test  = test_df[feat_cols].astype(np.float32).values\n",
        "    y_test  = test_df[\"label\"].values\n",
        "\n",
        "    results = []\n",
        "    for m in models:\n",
        "        if m == \"rf\":\n",
        "            clf = train_rf(df, feat_cols,\n",
        "                           split_col=split_col,\n",
        "                           train_value=train_value,\n",
        "                           test_value=test_value,\n",
        "                           **rf_params)\n",
        "            name = \"RandomForest\"\n",
        "        elif m == \"xgb\":\n",
        "            clf = train_xgb(df, feat_cols,\n",
        "                            split_col=split_col,\n",
        "                            train_value=train_value,\n",
        "                            test_value=test_value,\n",
        "                            **xgb_params)\n",
        "            name = \"XGBoost\"\n",
        "        elif m == \"mlp\":\n",
        "            clf = train_mlp(df, feat_cols,\n",
        "                            split_col=split_col,\n",
        "                            train_value=train_value,\n",
        "                            test_value=test_value,\n",
        "                            **mlp_params)\n",
        "            name = \"MLP\"\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        y_pred = clf.predict(X_test)\n",
        "        results.append({\n",
        "            \"model\":     name,\n",
        "            \"accuracy\":  accuracy_score(y_test, y_pred),\n",
        "            \"precision\": precision_score(y_test, y_pred),\n",
        "            \"recall\":    recall_score(y_test, y_pred),\n",
        "            \"f1\":        f1_score(y_test, y_pred),\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# example usage\n",
        "split = pd.read_csv(\"split_master.csv\")\n",
        "feat  = pd.read_csv(\"text_feats_fixed.csv\")\n",
        "df    = feat.merge(split, on=[\"file_id\", \"label\"])\n",
        "feat_cols = [c for c in df.columns if c.startswith(\"f\") and c[1:].isdigit()]\n",
        "\n",
        "results = train_fn(\n",
        "    df,\n",
        "    feat_cols,\n",
        "    models=[\"rf\", \"xgb\", \"mlp\"],\n",
        "    rf_params={\"n_estimators\":100, \"random_state\":23},\n",
        "    xgb_params={\"n_estimators\":100, \"max_depth\":4, \"learning_rate\":0.05, \"random_state\":23},\n",
        "    mlp_params={\"hidden_layer_sizes\":(100,), \"max_iter\":300, \"random_state\":23}\n",
        ")\n",
        "\n",
        "format_results(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wIhKw7_tYM1",
        "outputId": "39c5e98a-6479-45e7-9b2a-e8ec383ebc53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [01:30:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----------+-----------+--------+----------+\n",
            "|    Model     | Accuracy | Precision | Recall | F1-score |\n",
            "+--------------+----------+-----------+--------+----------+\n",
            "| RandomForest |  0.765   |   0.812   | 0.722  |  0.765   |\n",
            "|   XGBoost    |  0.824   |   0.833   | 0.833  |  0.833   |\n",
            "|     MLP      |  0.735   |   0.800   | 0.667  |  0.727   |\n",
            "+--------------+----------+-----------+--------+----------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}
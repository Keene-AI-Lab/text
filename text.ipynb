{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opensmile gdown transformers soundfile --quiet"
      ],
      "metadata": {
        "id": "eI40uUurY3lu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7bb04a4-a0b2-492e-9094-40c81e75c5ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/996.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m993.3/996.0 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.0/996.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.4/138.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accessing Dataset"
      ],
      "metadata": {
        "id": "nTrmLrrDYzF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio, display\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# install dataset using gdown\n",
        "!gdown 1p4ZQOwbHkD2RAvq2K5ekY5gcMi24XLnS\n",
        "\n",
        "tarball_filename = \"ADReSSo21-diagnosis-train.tar\"\n",
        "\n",
        "# extract contents from tarball\n",
        "with tarfile.open(tarball_filename, 'r:*') as tar:\n",
        "    tar.extractall(path=\"./\")\n",
        "\n",
        "# remove tarball after extraction\n",
        "os.remove(tarball_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pve1AehzY_WU",
        "outputId": "24e9aa36-f540-425e-8312-47b59f761f6c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1p4ZQOwbHkD2RAvq2K5ekY5gcMi24XLnS\n",
            "From (redirected): https://drive.google.com/uc?id=1p4ZQOwbHkD2RAvq2K5ekY5gcMi24XLnS&confirm=t&uuid=5cc67864-4e5e-4ef5-b19e-9a61902f2202\n",
            "To: /content/ADReSSo21-diagnosis-train.tar\n",
            "100% 1.75G/1.75G [00:23<00:00, 73.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "YZbMqh8ec5Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define base dataset directory and related paths\n",
        "dataset_dir = Path('ADReSSo21/diagnosis/train')\n",
        "paths = {\n",
        "    'audio': dataset_dir / 'audio',\n",
        "    'audio_ad': dataset_dir / 'audio' / 'ad',\n",
        "    'audio_cn': dataset_dir / 'audio' / 'cn',\n",
        "    'segmentation': dataset_dir / 'segmentation',\n",
        "    'segmentation_ad': dataset_dir / 'segmentation' / 'ad',\n",
        "    'segmentation_cn': dataset_dir / 'segmentation' / 'cn'\n",
        "}\n",
        "\n",
        "def count_files(directory):\n",
        "    \"\"\"return the number of files in the specified directory.\"\"\"\n",
        "    return len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])\n",
        "\n",
        "def plot_waveform_data(waveform, sr, title=\"Waveform\"):\n",
        "    \"\"\"plot the given waveform with its sampling rate.\"\"\"\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    librosa.display.waveshow(waveform, sr=sr)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"time (s)\")\n",
        "    plt.ylabel(\"amplitude\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_waveform(file_path=None, folder=paths['audio_ad']):\n",
        "    \"\"\"\n",
        "    load an audio file (default: first .wav in folder) and plot its waveform.\n",
        "    \"\"\"\n",
        "    if file_path is None:\n",
        "        wav_files = list(folder.glob(\"*.wav\"))\n",
        "        if not wav_files:\n",
        "            print(f\"no .wav files found in {folder}\")\n",
        "            return\n",
        "        file_path = wav_files[0]\n",
        "    else:\n",
        "        file_path = Path(file_path)\n",
        "\n",
        "    waveform, sr = librosa.load(str(file_path), sr=None)\n",
        "    print(\"original waveform shape:\", waveform.shape)\n",
        "    print(\"sampling rate:\", sr)\n",
        "    print(\"duration (seconds):\", len(waveform) / sr, \"\\n\")\n",
        "\n",
        "def load_audio_file(file_path: Path):\n",
        "    \"\"\"load and return the audio waveform and sample rate using soundfile.\"\"\"\n",
        "    return sf.read(file_path)\n",
        "\n",
        "def load_segmentation(seg_file: Path):\n",
        "    \"\"\"load and return the segmentation csv as a dataframe.\"\"\"\n",
        "    return pd.read_csv(seg_file)\n",
        "\n",
        "def extract_patient_segments(audio: np.ndarray, sr: int, seg_df: pd.DataFrame, speaker: str = \"PAR\"):\n",
        "    \"\"\"\n",
        "    extract patient segments from the audio where speaker equals the specified speaker.\n",
        "    returns:\n",
        "      - patient_mask: array with patient segments (nan elsewhere).\n",
        "      - concatenated: patient segments concatenated.\n",
        "      - segments: list of (begin, end) sample index tuples.\n",
        "    \"\"\"\n",
        "    patient_df = seg_df[seg_df['speaker'] == speaker]\n",
        "    patient_mask = np.full_like(audio, np.nan)\n",
        "    segments_list = []\n",
        "    segments = []\n",
        "\n",
        "    for _, row in patient_df.iterrows():\n",
        "        begin_sample = int(float(row['begin']) * sr / 1000)\n",
        "        end_sample = int(float(row['end']) * sr / 1000)\n",
        "        patient_mask[begin_sample:end_sample] = audio[begin_sample:end_sample]\n",
        "        segments_list.append(audio[begin_sample:end_sample])\n",
        "        segments.append((begin_sample, end_sample))\n",
        "\n",
        "    concatenated = np.concatenate(segments_list) if segments_list else np.array([])\n",
        "    return patient_mask, concatenated, segments\n",
        "\n",
        "def plot_overlay_waveform(audio: np.ndarray, sr: int, patient_mask: np.ndarray,\n",
        "                          orig_color: str = \"#0000FF\", patient_color: str = \"#00FF00\"):\n",
        "    \"\"\"plot the original waveform and overlay the patient-only segments.\"\"\"\n",
        "    time_axis = np.linspace(0, len(audio) / sr, num=len(audio))\n",
        "    plt.figure(figsize=(14, 4))\n",
        "    plt.plot(time_axis, audio, color=orig_color, label=\"Original\")\n",
        "    plt.plot(time_axis, patient_mask, color=patient_color, label=\"Patient-Only\")\n",
        "    plt.title(\"Original Audio with Patient Speech Highlighted\")\n",
        "    plt.xlabel(\"time (s)\")\n",
        "    plt.ylabel(\"amplitude\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def process_audio(audio_file: Path, seg_file: Path, plot: bool = False):\n",
        "    \"\"\"\n",
        "    load audio and segmentation files, extract patient segments,\n",
        "    optionally plot the overlay, and return processed data.\n",
        "    \"\"\"\n",
        "    audio, sr = load_audio_file(audio_file)\n",
        "    seg_df = load_segmentation(seg_file)\n",
        "    patient_mask, concatenated, segments = extract_patient_segments(audio, sr, seg_df)\n",
        "    if plot:\n",
        "        plot_overlay_waveform(audio, sr, patient_mask)\n",
        "    return audio, sr, patient_mask, concatenated, segments\n",
        "\n",
        "def init_transcriber(model_name: str = \"openai/whisper-large\", device: int = -1):\n",
        "    \"\"\"\n",
        "    initialize and return the automatic speech recognition pipeline using hugging face's transformers.\n",
        "\n",
        "    parameters:\n",
        "      model_name (str): the hugging face model name; default is \"openai/whisper-large\".\n",
        "      device (int): device to run the model on; -1 for cpu, or 0 (or other index) for gpu.\n",
        "\n",
        "    returns:\n",
        "      a hugging face asr pipeline instance.\n",
        "    \"\"\"\n",
        "    transcriber = pipeline(\"automatic-speech-recognition\", model=model_name, device=device)\n",
        "    return transcriber\n",
        "\n",
        "def transcribe_audio_file(file_path: Path, transcriber) -> str:\n",
        "    \"\"\"\n",
        "    transcribe a single audio file using the specified transcription pipeline.\n",
        "\n",
        "    parameters:\n",
        "      file_path (Path): path to the .wav audio file.\n",
        "      transcriber: a hugging face asr pipeline instance.\n",
        "\n",
        "    returns:\n",
        "      the transcription as a string.\n",
        "    \"\"\"\n",
        "    # note the addition of return_timestamps=true to support long audio files\n",
        "    transcription_result = transcriber(str(file_path), return_timestamps=True)\n",
        "    # retrieve the transcribed text from the result dictionary\n",
        "    return transcription_result.get(\"text\", \"\")\n",
        "\n",
        "def create_transcription_df(transcription_records: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    create a pandas dataframe from a list of transcription records.\n",
        "\n",
        "    each record in the list should be a dictionary with keys like 'file_name' and 'transcription'.\n",
        "\n",
        "    parameters:\n",
        "      transcription_records (list): a list of transcription record dictionaries.\n",
        "\n",
        "    returns:\n",
        "      dataframe containing the transcription data.\n",
        "    \"\"\"\n",
        "    return pd.DataFrame(transcription_records)\n"
      ],
      "metadata": {
        "id": "EcdkaN2QZJmG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transcription\n"
      ],
      "metadata": {
        "id": "3RKesCnlfOB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# initialize the transcription pipeline with the selected device\n",
        "transcriber = init_transcriber(model_name=\"openai/whisper-large\", device=device)\n",
        "\n",
        "# define the directory containing the audio files\n",
        "audio_dir = Path(\"ADReSSo21/diagnosis/train/audio/ad\")\n",
        "\n",
        "# get a sample .wav file; picks the first one in the directory\n",
        "sample_file = next(audio_dir.glob(\"*.wav\"), None)\n",
        "\n",
        "if sample_file is None:\n",
        "    print(f\"no .wav files found in {audio_dir}\")\n",
        "else:\n",
        "    print(\"transcribing file:\", sample_file.name)\n",
        "    # transcribe the audio file with return_timestamps enabled internally\n",
        "    transcription_text = transcribe_audio_file(sample_file, transcriber)\n",
        "    print(\"transcription:\")\n",
        "    print(transcription_text)\n",
        "\n",
        "    # create a transcription record and convert to a dataframe\n",
        "    transcription_record = [{\n",
        "        \"file_name\": sample_file.name,\n",
        "        \"transcription\": transcription_text\n",
        "    }]\n",
        "    df_transcriptions = create_transcription_df(transcription_record)\n",
        "\n",
        "    # save the dataframe as a csv file\n",
        "    csv_filename = \"transcriptions.csv\"\n",
        "    df_transcriptions.to_csv(csv_filename, index=False)\n",
        "    print(f\"transcription saved to {csv_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpAM6X0FfRgy",
        "outputId": "51dd5d5d-3386-461d-a5e8-9c7e827451f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing file: adrso049.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription:\n",
            " And there's the picture. All the action that you can see. The little boy climbing up in some cookers out the cooking jar. And his little sister reaching for some. And the little boy standing on the lye. and his big sister washing the dishes at the sink. Big sis washing the dishes and she got dishes sitting on the sink. I think she's running water. Can you tell me anything else that's going on over there? And I say the little sister's reach. Johnny Johnny he is he's up on the ladder, getting some cookies and. This is the region that we should reach it up after. Anything he's passing down to her. Okay, anything else. And now they're about to turn over. Okay. Okay. Anything else going on over here? The cups, maybe she's under washing, but she got them sitting on the sink. And maybe running water on the sink. When the guy is, you gotta carry the pool that you might get some light in there. Since the dishes stacked up, they might be on this thing. Anything else? Nothing but all right. Okay, good.\n",
            "Transcription saved to transcriptions.csv\n"
          ]
        }
      ]
    }
  ]
}
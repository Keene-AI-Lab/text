{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opensmile gdown transformers soundfile --quiet"
      ],
      "metadata": {
        "id": "eI40uUurY3lu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7bb04a4-a0b2-492e-9094-40c81e75c5ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/996.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m993.3/996.0 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.0/996.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.4/138.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accessing Dataset"
      ],
      "metadata": {
        "id": "nTrmLrrDYzF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio, display\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "# install dataset using gdown\n",
        "!gdown 1p4ZQOwbHkD2RAvq2K5ekY5gcMi24XLnS\n",
        "\n",
        "tarball_filename = \"ADReSSo21-diagnosis-train.tar\"\n",
        "\n",
        "# extract contents from tarball\n",
        "with tarfile.open(tarball_filename, 'r:*') as tar:\n",
        "    tar.extractall(path=\"./\")\n",
        "\n",
        "# remove tarball after extraction\n",
        "os.remove(tarball_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pve1AehzY_WU",
        "outputId": "24e9aa36-f540-425e-8312-47b59f761f6c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1p4ZQOwbHkD2RAvq2K5ekY5gcMi24XLnS\n",
            "From (redirected): https://drive.google.com/uc?id=1p4ZQOwbHkD2RAvq2K5ekY5gcMi24XLnS&confirm=t&uuid=5cc67864-4e5e-4ef5-b19e-9a61902f2202\n",
            "To: /content/ADReSSo21-diagnosis-train.tar\n",
            "100% 1.75G/1.75G [00:23<00:00, 73.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "YZbMqh8ec5Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = Path('ADReSSo21/diagnosis/train')\n",
        "paths = {\n",
        "    'audio': dataset_dir / 'audio',\n",
        "    'audio_ad': dataset_dir / 'audio' / 'ad',\n",
        "    'audio_cn': dataset_dir / 'audio' / 'cn',\n",
        "    'segmentation': dataset_dir / 'segmentation',\n",
        "    'segmentation_ad': dataset_dir / 'segmentation' / 'ad',\n",
        "    'segmentation_cn': dataset_dir / 'segmentation' / 'cn'\n",
        "}\n",
        "\n",
        "\n",
        "def load_audio_file(file_path: Path):\n",
        "    \"\"\"Load and return the audio waveform and sample rate using soundfile.\"\"\"\n",
        "    return sf.read(file_path)\n",
        "\n",
        "def load_segmentation(seg_file: Path):\n",
        "    \"\"\"Load and return the segmentation CSV as a Pandas DataFrame.\"\"\"\n",
        "    return pd.read_csv(seg_file)\n",
        "\n",
        "def extract_patient_segments(audio: np.ndarray, sr: int, seg_df: pd.DataFrame, speaker: str = \"PAR\"):\n",
        "    \"\"\"\n",
        "    Extract patient segments (where speaker == speaker) from the audio.\n",
        "\n",
        "    Returns:\n",
        "      - patient_mask: Array with patient segments (NaN elsewhere).\n",
        "      - concatenated: Patient segments concatenated into one array.\n",
        "      - segments: List of (begin, end) sample index tuples.\n",
        "    \"\"\"\n",
        "    patient_df = seg_df[seg_df['speaker'] == speaker]\n",
        "    patient_mask = np.full_like(audio, np.nan)\n",
        "    segments_list = []\n",
        "    segments = []\n",
        "\n",
        "    for _, row in patient_df.iterrows():\n",
        "        begin_sample = int(float(row['begin']) * sr / 1000)\n",
        "        end_sample = int(float(row['end']) * sr / 1000)\n",
        "        patient_mask[begin_sample:end_sample] = audio[begin_sample:end_sample]\n",
        "        segments_list.append(audio[begin_sample:end_sample])\n",
        "        segments.append((begin_sample, end_sample))\n",
        "\n",
        "    concatenated = np.concatenate(segments_list) if segments_list else np.array([])\n",
        "    return patient_mask, concatenated, segments\n",
        "\n",
        "def process_audio(audio_file: Path, seg_file: Path, plot: bool = False):\n",
        "    \"\"\"\n",
        "    Load an audio file and its corresponding segmentation CSV file,\n",
        "    extract patient segments (concatenated), and optionally plot the overlay.\n",
        "\n",
        "    Returns:\n",
        "      audio, sr, patient_mask, concatenated (patient-only audio), segments\n",
        "    \"\"\"\n",
        "    audio, sr = load_audio_file(audio_file)\n",
        "    seg_df = load_segmentation(seg_file)\n",
        "    patient_mask, concatenated, segments = extract_patient_segments(audio, sr, seg_df)\n",
        "\n",
        "    if plot:\n",
        "        time_axis = np.linspace(0, len(audio)/sr, num=len(audio))\n",
        "        plt.figure(figsize=(14, 4))\n",
        "        plt.plot(time_axis, audio, label=\"Original\")\n",
        "        plt.plot(time_axis, patient_mask, label=\"Patient-Only\")\n",
        "        plt.xlabel(\"Time (s)\")\n",
        "        plt.ylabel(\"Amplitude\")\n",
        "        plt.title(\"Patient Speech Isolation\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    return audio, sr, patient_mask, concatenated, segments\n",
        "\n",
        "def init_transcriber(model_name: str = \"openai/whisper-large\", device: int = -1):\n",
        "    \"\"\"\n",
        "    Initialize and return the automatic speech recognition pipeline\n",
        "    using Hugging Face's transformers.\n",
        "    \"\"\"\n",
        "    transcriber = pipeline(\"automatic-speech-recognition\", model=model_name, device=device)\n",
        "    return transcriber\n",
        "\n",
        "def transcribe_audio_file(file_path: str, transcriber) -> str:\n",
        "    \"\"\"\n",
        "    Transcribe an audio file given by its file path using the provided transcriber.\n",
        "\n",
        "    Parameters:\n",
        "      file_path (str): Path to the audio file.\n",
        "      transcriber: The initialized ASR pipeline.\n",
        "\n",
        "    Returns:\n",
        "      The transcription text.\n",
        "    \"\"\"\n",
        "    transcription_result = transcriber(file_path, return_timestamps=True)\n",
        "    return transcription_result.get(\"text\", \"\")\n",
        "\n",
        "def create_transcription_df(transcription_records: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a Pandas DataFrame from a list of transcription records.\n",
        "    \"\"\"\n",
        "    return pd.DataFrame(transcription_records)\n"
      ],
      "metadata": {
        "id": "EcdkaN2QZJmG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transcription\n"
      ],
      "metadata": {
        "id": "3RKesCnlfOB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # using cpu by setting device to 0\n",
        "    device = 0\n",
        "    transcriber = init_transcriber(model_name=\"openai/whisper-large\", device=device)\n",
        "\n",
        "    # pick a sample audio file from the audio_ad group\n",
        "    audio_file = next(paths['audio_ad'].glob(\"*.wav\"), None)\n",
        "    if audio_file is None:\n",
        "        raise FileNotFoundError(\"no audio file found in \" + str(paths['audio_ad']))\n",
        "\n",
        "    # construct the corresponding segmentation csv file path\n",
        "    seg_file = paths['segmentation_ad'] / f\"{audio_file.stem}.csv\"\n",
        "\n",
        "    # process audio to extract patient-only segments\n",
        "    audio, sr, patient_mask, concatenated, segments = process_audio(audio_file, seg_file, plot=False)\n",
        "\n",
        "    if concatenated.size == 0:\n",
        "        print(\"no patient speech segments found in the file.\")\n",
        "    else:\n",
        "        print(\"transcribing patient-only speech for file:\", audio_file.name)\n",
        "\n",
        "        # write the concatenated patient audio to a temporary wav file\n",
        "        import tempfile\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n",
        "            temp_filename = tmp_file.name\n",
        "        sf.write(temp_filename, concatenated, sr)\n",
        "\n",
        "        # transcribe using the temporary file\n",
        "        transcription_text = transcribe_audio_file(temp_filename, transcriber)\n",
        "        print(\"patient-only transcription:\")\n",
        "        print(transcription_text)\n",
        "\n",
        "        # remove the temporary file\n",
        "        os.remove(temp_filename)\n",
        "\n",
        "        # save the transcription in a csv file\n",
        "        record = [{\n",
        "            \"file_name\": audio_file.name,\n",
        "            \"transcription\": transcription_text\n",
        "        }]\n",
        "        df = create_transcription_df(record)\n",
        "        csv_filename = \"patient_transcriptions.csv\"\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "        print(f\"patient-only transcription saved to {csv_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpAM6X0FfRgy",
        "outputId": "063f7cc9-ef95-4543-ceb7-756bec158bdc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transcribing patient-only speech for file: adrso049.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patient-only transcription:\n",
            " The little boy climbing up in some cookers out the cooking job and his little sister reach of a song and they watch Dianna on the loud and it makes us to watch in the basement at the sink Big sis washing the dishes and she got dishes sitting on the sink. I think she's running water. And I say the little sister's reach. Johnny Johnny he's he's up on the ladder, get getting some cookies in the. This is the region that we should reach it up after. He's passing down to her. and the lather about to turn over. The cups, maybe she done washed them and she got them sitting on the sink. Maybe running water on the sink and she got a curry to pour. That she might get some light in there. Since the dishes stacked up, they might be on this thing. Nothing but alright.\n",
            "patient-only transcription saved to patient_transcriptions.csv\n"
          ]
        }
      ]
    }
  ]
}
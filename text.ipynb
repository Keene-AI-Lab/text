{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opensmile gdown transformers soundfile --quiet"
      ],
      "metadata": {
        "id": "eI40uUurY3lu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3d8af4-6198-48e9-b68d-c6b89476c0a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/996.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.6/996.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.0/996.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.4/138.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accessing Dataset"
      ],
      "metadata": {
        "id": "nTrmLrrDYzF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "import os\n",
        "import tempfile\n",
        "import concurrent.futures\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio, display\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# install dataset using gdown\n",
        "!gdown 1p4ZQOwbHkD2RAvq2K5ekY5gcMi24XLnS\n",
        "\n",
        "tarball_filename = \"ADReSSo21-diagnosis-train.tar\"\n",
        "\n",
        "# extract contents from tarball\n",
        "with tarfile.open(tarball_filename, 'r:*') as tar:\n",
        "    tar.extractall(path=\"./\")\n",
        "\n",
        "# remove tarball after extraction\n",
        "os.remove(tarball_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pve1AehzY_WU",
        "outputId": "0e0fd198-779c-4bb3-bdc3-fbdc4c82fb6c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1p4ZQOwbHkD2RAvq2K5ekY5gcMi24XLnS\n",
            "From (redirected): https://drive.google.com/uc?id=1p4ZQOwbHkD2RAvq2K5ekY5gcMi24XLnS&confirm=t&uuid=b3ddf379-a455-4aa3-a747-bf6402e534b9\n",
            "To: /content/ADReSSo21-diagnosis-train.tar\n",
            "100% 1.75G/1.75G [00:18<00:00, 92.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio, display\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import warnings\n",
        "\n",
        "# -------------------------------------------------------------------------------\n",
        "# Warning Suppression: Authentication and Whisper Deprecation Warnings\n",
        "# -------------------------------------------------------------------------------\n",
        "warnings.filterwarnings(\"ignore\", message=\"Error while fetching `HF_TOKEN` secret value from your vault\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"Due to a bug fix in\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"The input name `inputs` is deprecated\", category=FutureWarning)\n",
        "\n",
        "# -------------------------------------------------------------------------------\n",
        "# Helper Functions\n",
        "# -------------------------------------------------------------------------------\n",
        "\n",
        "def get_dataset_paths(dataset_root: str = \"ADReSSo21/diagnosis/train\"):\n",
        "    \"\"\"\n",
        "    Return a dictionary with paths for audio and segmentation files.\n",
        "    \"\"\"\n",
        "    dataset_dir = Path(dataset_root)\n",
        "    return {\n",
        "        \"audio\": dataset_dir / \"audio\",\n",
        "        \"audio_ad\": dataset_dir / \"audio\" / \"ad\",\n",
        "        \"audio_cn\": dataset_dir / \"audio\" / \"cn\",\n",
        "        \"segmentation\": dataset_dir / \"segmentation\",\n",
        "        \"segmentation_ad\": dataset_dir / \"segmentation\" / \"ad\",\n",
        "        \"segmentation_cn\": dataset_dir / \"segmentation\" / \"cn\"\n",
        "    }\n",
        "\n",
        "def load_audio_file(file_path: Path):\n",
        "    \"\"\"Load audio waveform and sampling rate using soundfile.\"\"\"\n",
        "    return sf.read(file_path)\n",
        "\n",
        "def load_segmentation(seg_file: Path):\n",
        "    \"\"\"Load the segmentation CSV into a Pandas DataFrame.\"\"\"\n",
        "    return pd.read_csv(seg_file)\n",
        "\n",
        "def extract_patient_segments(audio: np.ndarray, sr: int, seg_df: pd.DataFrame, speaker: str = \"PAR\"):\n",
        "    \"\"\"\n",
        "    Extract patient segments from the audio as specified in the segmentation CSV.\n",
        "\n",
        "    Returns:\n",
        "      - patient_mask: An array with patient segments (NaN elsewhere).\n",
        "      - concatenated: Patient segments concatenated into one array.\n",
        "      - segments: List of (begin, end) sample index tuples.\n",
        "    \"\"\"\n",
        "    patient_df = seg_df[seg_df[\"speaker\"] == speaker]\n",
        "    patient_mask = np.full_like(audio, np.nan)\n",
        "    segments_list = []\n",
        "    segments = []\n",
        "    for _, row in patient_df.iterrows():\n",
        "        begin_sample = int(float(row[\"begin\"]) * sr / 1000)\n",
        "        end_sample = int(float(row[\"end\"]) * sr / 1000)\n",
        "        patient_mask[begin_sample:end_sample] = audio[begin_sample:end_sample]\n",
        "        segments_list.append(audio[begin_sample:end_sample])\n",
        "        segments.append((begin_sample, end_sample))\n",
        "    concatenated = np.concatenate(segments_list) if segments_list else np.array([])\n",
        "    return patient_mask, concatenated, segments\n",
        "\n",
        "def process_audio(audio_file: Path, seg_file: Path, plot: bool = False):\n",
        "    \"\"\"\n",
        "    Load an audio file and its corresponding segmentation CSV,\n",
        "    extract the patient-only segments, and optionally plot the waveform.\n",
        "\n",
        "    Returns:\n",
        "      audio, sr, patient_mask, concatenated (patient-only audio), segments.\n",
        "    \"\"\"\n",
        "    audio, sr = load_audio_file(audio_file)\n",
        "    seg_df = load_segmentation(seg_file)\n",
        "    patient_mask, concatenated, segments = extract_patient_segments(audio, sr, seg_df)\n",
        "    if plot:\n",
        "        time_axis = np.linspace(0, len(audio) / sr, num=len(audio))\n",
        "        plt.figure(figsize=(14, 4))\n",
        "        plt.plot(time_axis, audio, label=\"Original\")\n",
        "        plt.plot(time_axis, patient_mask, label=\"Patient-Only\")\n",
        "        plt.xlabel(\"Time (s)\")\n",
        "        plt.ylabel(\"Amplitude\")\n",
        "        plt.title(\"Patient Speech Isolation\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    return audio, sr, patient_mask, concatenated, segments\n",
        "\n",
        "def init_transcriber(model_name: str = \"openai/whisper-large\", device: int = 0):\n",
        "    \"\"\"\n",
        "    Initialize the Hugging Face ASR pipeline.\n",
        "\n",
        "    Set device to 0 to use your GPU.\n",
        "    \"\"\"\n",
        "    return pipeline(\"automatic-speech-recognition\", model=model_name, device=device)\n",
        "\n",
        "def transcribe_audio_file(file_path: str, transcriber) -> str:\n",
        "    \"\"\"\n",
        "    Transcribe an audio file given its file path.\n",
        "\n",
        "    Returns the transcription text.\n",
        "    \"\"\"\n",
        "    result = transcriber(file_path, return_timestamps=True)\n",
        "    return result.get(\"text\", \"\")\n",
        "\n",
        "def create_transcription_df(transcription_records: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a Pandas DataFrame from a list of transcription records.\n",
        "    \"\"\"\n",
        "    return pd.DataFrame(transcription_records)\n",
        "\n",
        "# -------------------------------------------------------------------------------\n",
        "# Main Function: Get Transcripts for 30 Files on GPU\n",
        "# -------------------------------------------------------------------------------\n",
        "def get_transcripts():\n",
        "    \"\"\"\n",
        "    Process 30 audio files from the dataset (from both AD and CN groups) sequentially on GPU,\n",
        "    print the word count for each transcript, and save the sorted transcripts to a CSV file.\n",
        "    \"\"\"\n",
        "    paths = get_dataset_paths()\n",
        "    device = 0  # Use GPU (device index 0)\n",
        "    model_name = \"openai/whisper-large\"\n",
        "\n",
        "    # Gather audio files from both AD and CN groups, sort by file name, and select the first 30\n",
        "    audio_files = sorted(list(paths[\"audio_ad\"].glob(\"*.wav\")) + list(paths[\"audio_cn\"].glob(\"*.wav\")), key=lambda f: f.name)[:30]\n",
        "    print(f\"Processing {len(audio_files)} audio files sequentially on GPU.\")\n",
        "\n",
        "    transcripts = []\n",
        "    transcriber = init_transcriber(model_name=model_name, device=device)  # Initialize transcriber once\n",
        "    for audio_file in audio_files:\n",
        "        # Determine segmentation CSV based on the parent folder\n",
        "        if \"ad\" in audio_file.parent.name.lower():\n",
        "            seg_file = paths[\"segmentation_ad\"] / f\"{audio_file.stem}.csv\"\n",
        "        else:\n",
        "            seg_file = paths[\"segmentation_cn\"] / f\"{audio_file.stem}.csv\"\n",
        "\n",
        "        # Process audio to extract the patient-only audio\n",
        "        audio, sr, patient_mask, concatenated, segments = process_audio(audio_file, seg_file, plot=False)\n",
        "        if concatenated.size == 0:\n",
        "            transcript = \"No patient speech segments found.\"\n",
        "        else:\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n",
        "                temp_filename = tmp_file.name\n",
        "            sf.write(temp_filename, concatenated, sr)\n",
        "            transcript = transcribe_audio_file(temp_filename, transcriber)\n",
        "            os.remove(temp_filename)\n",
        "        transcripts.append({\"file_name\": audio_file.name, \"transcription\": transcript})\n",
        "        word_count = len(transcript.split())\n",
        "        print(f\"File '{audio_file.name}': {word_count} words in transcript.\")\n",
        "\n",
        "    # Ensure the list of transcripts is sorted by file name (it should already be sorted but this guarantees it)\n",
        "    transcripts = sorted(transcripts, key=lambda x: x[\"file_name\"])\n",
        "\n",
        "    df = create_transcription_df(transcripts)\n",
        "    csv_filename = \"sorted_patient_transcriptions.csv\"\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"All sorted transcriptions saved to {csv_filename}\")"
      ],
      "metadata": {
        "id": "ZHFQbN73BUaq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transcription\n"
      ],
      "metadata": {
        "id": "3RKesCnlfOB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_transcripts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpAM6X0FfRgy",
        "outputId": "e1b2c97a-a55a-4b1c-ebe8-834e9ddddcb3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 30 audio files sequentially on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n",
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso002.wav': 171 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso003.wav': 82 words in transcript.\n",
            "File 'adrso005.wav': 141 words in transcript.\n",
            "File 'adrso007.wav': 63 words in transcript.\n",
            "File 'adrso008.wav': 137 words in transcript.\n",
            "File 'adrso010.wav': 41 words in transcript.\n",
            "File 'adrso012.wav': 137 words in transcript.\n",
            "File 'adrso014.wav': 138 words in transcript.\n",
            "File 'adrso015.wav': 99 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso016.wav': 79 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso017.wav': 117 words in transcript.\n",
            "File 'adrso018.wav': 5 words in transcript.\n",
            "File 'adrso019.wav': 5 words in transcript.\n",
            "File 'adrso021.wav': 5 words in transcript.\n",
            "File 'adrso022.wav': 5 words in transcript.\n",
            "File 'adrso023.wav': 5 words in transcript.\n",
            "File 'adrso024.wav': 182 words in transcript.\n",
            "File 'adrso025.wav': 195 words in transcript.\n",
            "File 'adrso027.wav': 83 words in transcript.\n",
            "File 'adrso028.wav': 47 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso031.wav': 90 words in transcript.\n",
            "File 'adrso032.wav': 184 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso033.wav': 72 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso035.wav': 84 words in transcript.\n",
            "File 'adrso036.wav': 40 words in transcript.\n",
            "File 'adrso039.wav': 28 words in transcript.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'adrso043.wav': 250 words in transcript.\n",
            "File 'adrso045.wav': 291 words in transcript.\n",
            "File 'adrso046.wav': 108 words in transcript.\n",
            "File 'adrso047.wav': 94 words in transcript.\n",
            "All sorted transcriptions saved to sorted_patient_transcriptions.csv\n"
          ]
        }
      ]
    }
  ]
}